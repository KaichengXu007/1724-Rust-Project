# ğŸš€ Rust LLM Inference Service

A high-performance, production-ready Large Language Model (LLM) inference service built with Rust backend and React frontend. Provides OpenAI-compatible APIs with real-time token streaming, session management, and a modern web UI.

[![Rust](https://img.shields.io/badge/rust-1.75%2B-orange.svg)](https://www.rust-lang.org/)
[![React](https://img.shields.io/badge/react-19.2.0-blue.svg)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/typescript-5.x-blue.svg)](https://www.typescriptlang.org/)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](docker/)

## âœ¨ Features

### ğŸ¯ Core Capabilities
- **Multiple Model Support**: Load and manage multiple Huggingface-format models via Candle and Mistral.rs
- **Streaming Inference**: Real-time token streaming via WebSocket with tokens/second display
- **Session Management**: Stateful conversations with full history and session switching
  - SQLite-backed persistence with per-session durability
- **Modern React UI**: 
  - Built with React 19 + TypeScript + Vite
  - Zustand state management
  - Tailwind CSS v3 for styling
  - Real-time markdown rendering with syntax highlighting
  - Code copy buttons and dark mode
  - Session history with export functionality
  - Advanced model settings panel

### ğŸ”’ Security & Governance
- **API Key Authentication**: Optional token-based authentication
- **Rate Limiting**: Per-key and IP-based rate limiting
- **Content Validation**: Configurable prompt/response length guards
- **CORS Support**: Cross-origin resource sharing configuration

### ğŸ“Š Observability
- **Prometheus Metrics**: Built-in metrics for latency, throughput, and token counts
- **Structured Logging**: Configurable log levels with tracing
- **Health Probes**: `/health` and `/readiness` endpoints for orchestration
- **Performance Tracking**: Inference time, tokens/second, cache hits

### ğŸš¢ Deployment
- **Single Binary**: Portable executable with zero runtime dependencies
- **Docker Support**: Multi-stage builds for CPU and CUDA
- **Docker Compose**: Ready-to-use orchestration with Prometheus/Grafana
- **Configuration**: TOML-based config with sensible defaults

### ğŸ”Œ API Compatibility
- **OpenAI-style Endpoints**: `/completions`, `/chat/completions`
- **WebSocket Chat**: Real-time bidirectional streaming
- **Model Registry**: List, query, and manage models
- **RESTful Design**: Standard HTTP methods and status codes

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                React Frontend (TypeScript)                  â”‚
â”‚   Vite â€¢ Zustand â€¢ Tailwind CSS â€¢ React Markdown           â”‚
â”‚   WebSocket â€¢ Code Highlighting â€¢ Session Management        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/WS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Axum Web Framework                       â”‚
â”‚            Routes â€¢ Middleware â€¢ State Management            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Authentication  â”‚  Rate Limiting  â”‚  Content Validation    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               Session Manager (HashMap + Mutex)             â”‚
â”‚             Conversation History â€¢ Context Pruning           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              M1 Engine Adapter (mistral.rs)                 â”‚
â”‚         Model Loader â€¢ Tokenization â€¢ Sampling              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Candle ML Framework                       â”‚
â”‚              CUDA â€¢ Metal â€¢ CPU â€¢ Quantization              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components**:
- **Frontend**:
  - **`App.tsx`**: Root component with session lifecycle management
  - **`Sidebar.tsx`**: Session list, settings panel, export functionality
  - **`ChatContainer.tsx`**: Main chat interface with auto-scroll
  - **`Message.tsx`**: Markdown rendering with syntax highlighting
  - **`chatStore.ts`**: Zustand state management (12 actions)
  - **`api.ts`**: API service layer with WebSocket support
  - **`useWebSocket.ts`**: WebSocket hook for streaming
- **Backend**:
  - **`routes.rs`**: HTTP endpoints, WebSocket handlers
  - **`engine.rs`**: Inference abstraction, model management
  - **`state.rs`**: Application state, session persistence
  - **`server.rs`**: Entry point with model pre-warming

---

## ğŸš€ Quick Start

### Prerequisites
- **Rust** 1.75+ (`rustup` recommended)
- **Node.js** 18+ and npm (for frontend development)
- **(Optional)** NVIDIA GPU + CUDA Toolkit 12.1+ for GPU acceleration
- **(Optional)** Docker for containerized deployment

### Installation

1. **Clone the repository**:
```bash
git clone https://github.com/KaichengXu007/1724-Rust-Project.git
cd 1724-Rust-Project
```

2. **Build the frontend**:
```bash
cd frontend
npm install
npm run build
cd ..
```

3. **Create configuration** (optional):
```bash
cp config.example.toml config.toml
# Edit config.toml to customize settings
```

4. **Run the service**:

**CPU Mode**:
```bash
cargo run --release --bin server
```

**GPU Mode (CUDA)**:
```bash
cargo run --release --features cuda --bin server
```

5. **Access the web UI**:
Open your browser to `http://localhost:3000`

### Frontend Development

To run the frontend in development mode with hot reload:

```bash
cd frontend
npm run dev
```

Then run the backend server separately. The Vite dev server will proxy API requests to the backend.

---

## ğŸ“ Configuration

The service uses a TOML configuration file (`config.toml`). See `config.example.toml` for full options.

### Key Settings

```toml
[server]
host = "127.0.0.1"
port = 3000
log_level = "info"

[models]
default_device = "cuda"  # cuda, cpu, metal
max_concurrent_requests = 10

[[models.available_models]]
id = "qwen"
name = "Qwen/Qwen2.5-0.5B-Instruct"
context_length = 4096

[security]
enable_auth = false  # Set true to require API keys

[limits]
max_prompt_length = 8192
max_response_tokens = 2048
max_sessions = 1000
default_rate_limit_per_minute = 60
```

### Environment Variables

- `RUST_LOG`: Override log level (e.g., `debug`, `trace`)
- `CUDA_VISIBLE_DEVICES`: Select GPU device (e.g., `0`)

---

## ğŸ”Œ API Usage

See [API Reference](docs/API_REFERENCE.md) for comprehensive documentation.

### Examples

**List models**:
```bash
curl http://localhost:3000/models
```

**Generate completion**:
```bash
curl -X POST http://localhost:3000/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "Explain Rust ownership in one sentence:",
    "max_tokens": 50
  }'
```

**Chat with streaming**:
```bash
curl -X POST http://localhost:3000/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model-name": "Qwen/Qwen2.5-0.5B-Instruct",
    "prompt": "What is async/await?",
    "max-token": 256,
    "device": "cuda"
  }'
```

---

## ğŸ§ª Testing

Run the test suite:

```bash
# All tests
cargo test
```

---

## ğŸ“Š Monitoring

### Prometheus Metrics

Access metrics at `http://localhost:3000/metrics`

**Key Metrics**:
- `chat_completions_requests_total`: Request count
- `chat_inference_duration_seconds`: Inference latency
- `chat_generated_tokens_total`: Token throughput
- `completions_errors_total`: Error rate

### Health Checks

```bash
# Liveness
curl http://localhost:3000/health

# Readiness (checks model availability)
curl http://localhost:3000/readiness
```
---

### Rate Limiting

- **Default**: 60 requests/minute per IP
- **Per-key**: Configurable in `config.toml`
- **Enforcement**: Automatic via middleware

---

## ğŸ› ï¸ Development

### Project Structure

```
.
â”œâ”€â”€ frontend/                   # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatContainer.tsx  # Main chat UI
â”‚   â”‚   â”‚   â”œâ”€â”€ Message.tsx        # Message rendering
â”‚   â”‚   â”‚   â””â”€â”€ Sidebar.tsx        # Session & settings
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts    # WebSocket streaming
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts             # API client
â”‚   â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â”‚   â””â”€â”€ chatStore.ts       # Zustand state
â”‚   â”‚   â”œâ”€â”€ App.tsx                # Root component
â”‚   â”‚   â””â”€â”€ index.css              # Tailwind styles
â”‚   â”œâ”€â”€ dist/                   # Production build
â”‚   â”œâ”€â”€ package.json            # Dependencies
â”‚   â”œâ”€â”€ tsconfig.json           # TypeScript config
â”‚   â”œâ”€â”€ tailwind.config.js      # Tailwind config
â”‚   â””â”€â”€ vite.config.ts          # Vite config
â”œâ”€â”€ src/                        # Rust backend
â”‚   â”œâ”€â”€ bin/
â”‚   â”‚   â””â”€â”€ server.rs           # Entry point
â”‚   â”œâ”€â”€ config.rs               # Configuration
â”‚   â”œâ”€â”€ engine.rs               # Inference engine
â”‚   â”œâ”€â”€ engine_mock.rs          # Test mock
â”‚   â”œâ”€â”€ lib.rs                  # Library root
â”‚   â”œâ”€â”€ models.rs               # Data models
â”‚   â”œâ”€â”€ routes.rs               # HTTP handlers
â”‚   â””â”€â”€ state.rs                # Application state
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ integration_tests.rs    # API tests
â”‚   â”œâ”€â”€ config_tests.rs         # Config tests
â”‚   â””â”€â”€ middleware_tests.rs     # Middleware tests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API_REFERENCE.md        # API documentation
â”‚   â””â”€â”€ PROJECT_DOCUMENTATION.md # Complete guide
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile              # CPU build
â”‚   â”œâ”€â”€ Dockerfile.cuda         # GPU build
â”‚   â”œâ”€â”€ docker-compose.yml      # Orchestration
â”‚   â”œâ”€â”€ prometheus.yml          # Metrics config
â”‚   â””â”€â”€ README.md               # Docker guide
â”œâ”€â”€ Cargo.toml                  # Rust dependencies
â”œâ”€â”€ config.example.toml         # Config template
â””â”€â”€ postman_collection.json     # API tests
```

### Adding a New Model

1. Edit `config.toml`:
```toml
[[models.available_models]]
id = "llama"
name = "meta-llama/Llama-3.2-1B"
context_length = 8192
```

2. Restart the service
3. Model loads lazily on first request

### Building Features

**CUDA Support**:
```bash
cargo build --release --features cuda
```

**Metal Support (macOS)**:
```bash
cargo build --release --features metal
```

**Flash Attention**:
```bash
cargo build --release --features flash-attn
```

---

## ğŸ“š Additional Resources

- **[API Reference](docs/API_REFERENCE.md)**: Complete endpoint documentation
- **[Project Documentation](docs/PROJECT_DOCUMENTATION.md)**: Full setup and deployment guide
- **[Docker Guide](docker/README.md)**: Container deployment instructions
- **[Postman Collection](postman_collection.json)**: Ready-to-use API tests
- **[mistral.rs](https://github.com/EricLBuehler/mistral.rs)**: Underlying inference engine
- **[Candle](https://github.com/huggingface/candle)**: ML framework

---

## ğŸ™ Acknowledgments

- **[mistral.rs](https://github.com/EricLBuehler/mistral.rs)**: High-performance Rust inference
- **[Candle](https://github.com/huggingface/candle)**: Minimalist ML framework
- **[Axum](https://github.com/tokio-rs/axum)**: Ergonomic web framework
- **[React](https://reactjs.org/)**: UI library for building interactive interfaces
- **[Vite](https://vitejs.dev/)**: Next-generation frontend tooling
- **[Zustand](https://github.com/pmndrs/zustand)**: Simple state management
- **[Tailwind CSS](https://tailwindcss.com/)**: Utility-first CSS framework
- **Rust & TypeScript Communities**: For amazing tooling and libraries

---
